{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, r2_score, accuracy_score, precision_score\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollatorWithPadding, BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, BertModel, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_qual_dir = \"..\\\\IBM_Debater_arg_quality\\\\arg_quality_rank_30k.csv\"\n",
    "\n",
    "arg_qual_df = pd.read_csv(arg_qual_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>argument</th>\n",
       "      <th>topic</th>\n",
       "      <th>set</th>\n",
       "      <th>WA</th>\n",
       "      <th>MACE-P</th>\n",
       "      <th>stance_WA</th>\n",
       "      <th>stance_WA_conf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"marriage\" isn't keeping up with the times.  a...</td>\n",
       "      <td>We should abandon marriage</td>\n",
       "      <td>train</td>\n",
       "      <td>0.846165</td>\n",
       "      <td>0.297659</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.a multi-party system would be too confusing a...</td>\n",
       "      <td>We should adopt a multi-party system</td>\n",
       "      <td>train</td>\n",
       "      <td>0.891271</td>\n",
       "      <td>0.726133</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\ero-tolerance policy in schools should not be...</td>\n",
       "      <td>We should adopt a zero-tolerance policy in sch...</td>\n",
       "      <td>dev</td>\n",
       "      <td>0.721192</td>\n",
       "      <td>0.396953</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>`people reach their limit when it comes to the...</td>\n",
       "      <td>Assisted suicide should be a criminal offence</td>\n",
       "      <td>train</td>\n",
       "      <td>0.730395</td>\n",
       "      <td>0.225212</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100% agree, should they do that, it would be a...</td>\n",
       "      <td>We should abolish safe spaces</td>\n",
       "      <td>train</td>\n",
       "      <td>0.236686</td>\n",
       "      <td>0.004104</td>\n",
       "      <td>1</td>\n",
       "      <td>0.805517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            argument  \\\n",
       "0  \"marriage\" isn't keeping up with the times.  a...   \n",
       "1  .a multi-party system would be too confusing a...   \n",
       "2  \\ero-tolerance policy in schools should not be...   \n",
       "3  `people reach their limit when it comes to the...   \n",
       "4  100% agree, should they do that, it would be a...   \n",
       "\n",
       "                                               topic    set        WA  \\\n",
       "0                         We should abandon marriage  train  0.846165   \n",
       "1               We should adopt a multi-party system  train  0.891271   \n",
       "2  We should adopt a zero-tolerance policy in sch...    dev  0.721192   \n",
       "3      Assisted suicide should be a criminal offence  train  0.730395   \n",
       "4                      We should abolish safe spaces  train  0.236686   \n",
       "\n",
       "     MACE-P  stance_WA  stance_WA_conf  \n",
       "0  0.297659          1        1.000000  \n",
       "1  0.726133         -1        1.000000  \n",
       "2  0.396953         -1        1.000000  \n",
       "3  0.225212         -1        1.000000  \n",
       "4  0.004104          1        0.805517  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train test dev\n",
    "arg_qual_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30497"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(arg_qual_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_qual_df.dropna(inplace=True)\n",
    "arg_qual_df.drop_duplicates(inplace=True)\n",
    "arg_qual_df.drop(columns=[\"MACE-P\", \"stance_WA\", \"stance_WA_conf\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc the td-idf score of the text\n",
    "\n",
    "def get_most_important_word(df, feature):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(df[feature])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    most_important_words = []\n",
    "    \n",
    "    for row in X:\n",
    "        row_data = row.toarray().flatten()\n",
    "        most_important_index = row_data.argmax()\n",
    "        most_important_word = feature_names[most_important_index]\n",
    "        \n",
    "        if most_important_word.isnumeric():\n",
    "            most_important_words.append(\"number\")\n",
    "        elif most_important_word.isalpha():\n",
    "            most_important_words.append(most_important_word)\n",
    "        else:\n",
    "            most_important_words.append(\"na\")\n",
    "    return most_important_words\n",
    "\n",
    "arg_qual_df['most_important_word'] = get_most_important_word(arg_qual_df, 'argument')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>argument</th>\n",
       "      <th>topic</th>\n",
       "      <th>set</th>\n",
       "      <th>WA</th>\n",
       "      <th>most_important_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"marriage\" isn't keeping up with the times.  a...</td>\n",
       "      <td>We should abandon marriage</td>\n",
       "      <td>train</td>\n",
       "      <td>0.846165</td>\n",
       "      <td>incorporates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.a multi-party system would be too confusing a...</td>\n",
       "      <td>We should adopt a multi-party system</td>\n",
       "      <td>train</td>\n",
       "      <td>0.891271</td>\n",
       "      <td>consensus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\ero-tolerance policy in schools should not be...</td>\n",
       "      <td>We should adopt a zero-tolerance policy in sch...</td>\n",
       "      <td>dev</td>\n",
       "      <td>0.721192</td>\n",
       "      <td>nuanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>`people reach their limit when it comes to the...</td>\n",
       "      <td>Assisted suicide should be a criminal offence</td>\n",
       "      <td>train</td>\n",
       "      <td>0.730395</td>\n",
       "      <td>suffering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100% agree, should they do that, it would be a...</td>\n",
       "      <td>We should abolish safe spaces</td>\n",
       "      <td>train</td>\n",
       "      <td>0.236686</td>\n",
       "      <td>number</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            argument  \\\n",
       "0  \"marriage\" isn't keeping up with the times.  a...   \n",
       "1  .a multi-party system would be too confusing a...   \n",
       "2  \\ero-tolerance policy in schools should not be...   \n",
       "3  `people reach their limit when it comes to the...   \n",
       "4  100% agree, should they do that, it would be a...   \n",
       "\n",
       "                                               topic    set        WA  \\\n",
       "0                         We should abandon marriage  train  0.846165   \n",
       "1               We should adopt a multi-party system  train  0.891271   \n",
       "2  We should adopt a zero-tolerance policy in sch...    dev  0.721192   \n",
       "3      Assisted suicide should be a criminal offence  train  0.730395   \n",
       "4                      We should abolish safe spaces  train  0.236686   \n",
       "\n",
       "  most_important_word  \n",
       "0        incorporates  \n",
       "1           consensus  \n",
       "2             nuanced  \n",
       "3           suffering  \n",
       "4              number  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_qual_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_words(sentence):\n",
    "    sen_normal = sentence.lower().strip()\n",
    "    for word_idx in range(len(sen_normal)):\n",
    "        if not sen_normal[word_idx] in \" abcdefghijklmnopqrstuvwxyz-',.\":\n",
    "            sen_normal = sen_normal.replace(sen_normal[word_idx], \"*\")\n",
    "    sen_normal = sen_normal.replace(\"*\", \"\")\n",
    "    return len(sen_normal.split())\n",
    "\n",
    "arg_qual_df['word_count'] = arg_qual_df['argument'].apply(count_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>argument</th>\n",
       "      <th>topic</th>\n",
       "      <th>set</th>\n",
       "      <th>WA</th>\n",
       "      <th>most_important_word</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"marriage\" isn't keeping up with the times.  a...</td>\n",
       "      <td>We should abandon marriage</td>\n",
       "      <td>train</td>\n",
       "      <td>0.846165</td>\n",
       "      <td>incorporates</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.a multi-party system would be too confusing a...</td>\n",
       "      <td>We should adopt a multi-party system</td>\n",
       "      <td>train</td>\n",
       "      <td>0.891271</td>\n",
       "      <td>consensus</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\ero-tolerance policy in schools should not be...</td>\n",
       "      <td>We should adopt a zero-tolerance policy in sch...</td>\n",
       "      <td>dev</td>\n",
       "      <td>0.721192</td>\n",
       "      <td>nuanced</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>`people reach their limit when it comes to the...</td>\n",
       "      <td>Assisted suicide should be a criminal offence</td>\n",
       "      <td>train</td>\n",
       "      <td>0.730395</td>\n",
       "      <td>suffering</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100% agree, should they do that, it would be a...</td>\n",
       "      <td>We should abolish safe spaces</td>\n",
       "      <td>train</td>\n",
       "      <td>0.236686</td>\n",
       "      <td>number</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            argument  \\\n",
       "0  \"marriage\" isn't keeping up with the times.  a...   \n",
       "1  .a multi-party system would be too confusing a...   \n",
       "2  \\ero-tolerance policy in schools should not be...   \n",
       "3  `people reach their limit when it comes to the...   \n",
       "4  100% agree, should they do that, it would be a...   \n",
       "\n",
       "                                               topic    set        WA  \\\n",
       "0                         We should abandon marriage  train  0.846165   \n",
       "1               We should adopt a multi-party system  train  0.891271   \n",
       "2  We should adopt a zero-tolerance policy in sch...    dev  0.721192   \n",
       "3      Assisted suicide should be a criminal offence  train  0.730395   \n",
       "4                      We should abolish safe spaces  train  0.236686   \n",
       "\n",
       "  most_important_word  word_count  \n",
       "0        incorporates          27  \n",
       "1           consensus          18  \n",
       "2             nuanced          31  \n",
       "3           suffering          40  \n",
       "4              number          11  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_qual_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7913285945189035"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_qual_score(df, feature):\n",
    "    df_len = len(df)\n",
    "    df_sum = df[feature].sum()\n",
    "    return df_sum / df_len\n",
    "\n",
    "avg_qual_score(arg_qual_df, \"WA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amount_high_low_qual(df, feature, threshold):\n",
    "    high_qual = df[df[feature] > threshold]\n",
    "    low_qual = df[df[feature] < threshold]\n",
    "    return f\"high qual: {len(high_qual)}, low qual: {len(low_qual)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_qual_df = arg_qual_df[arg_qual_df[\"WA\"] < 0.7]\n",
    "higher_qual_df = arg_qual_df[arg_qual_df[\"WA\"] > 0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df = pd.concat([lower_qual_df, higher_qual_df.sample(n=len(lower_qual_df), random_state=32)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>argument</th>\n",
       "      <th>topic</th>\n",
       "      <th>set</th>\n",
       "      <th>WA</th>\n",
       "      <th>most_important_word</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100% agree, should they do that, it would be a...</td>\n",
       "      <td>We should abolish safe spaces</td>\n",
       "      <td>train</td>\n",
       "      <td>0.236686</td>\n",
       "      <td>number</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a bad score in an intelligence test is a blow ...</td>\n",
       "      <td>Intelligence tests bring more harm than good</td>\n",
       "      <td>dev</td>\n",
       "      <td>0.638716</td>\n",
       "      <td>blow</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A ban would be inffective, people who want a b...</td>\n",
       "      <td>Surrogacy should be banned</td>\n",
       "      <td>train</td>\n",
       "      <td>0.467092</td>\n",
       "      <td>foresaken</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Blockade is the perfect way to create stagna...</td>\n",
       "      <td>Blockade of the Gaza Strip should be ended</td>\n",
       "      <td>dev</td>\n",
       "      <td>0.555555</td>\n",
       "      <td>stagnation</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A blockade is what you do when you want any ch...</td>\n",
       "      <td>Blockade of the Gaza Strip should be ended</td>\n",
       "      <td>dev</td>\n",
       "      <td>0.634427</td>\n",
       "      <td>you</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            argument  \\\n",
       "0  100% agree, should they do that, it would be a...   \n",
       "1  a bad score in an intelligence test is a blow ...   \n",
       "2  A ban would be inffective, people who want a b...   \n",
       "3  A Blockade is the perfect way to create stagna...   \n",
       "4  A blockade is what you do when you want any ch...   \n",
       "\n",
       "                                          topic    set        WA  \\\n",
       "0                 We should abolish safe spaces  train  0.236686   \n",
       "1  Intelligence tests bring more harm than good    dev  0.638716   \n",
       "2                    Surrogacy should be banned  train  0.467092   \n",
       "3    Blockade of the Gaza Strip should be ended    dev  0.555555   \n",
       "4    Blockade of the Gaza Strip should be ended    dev  0.634427   \n",
       "\n",
       "  most_important_word  word_count  \n",
       "0              number          11  \n",
       "1                blow          25  \n",
       "2           foresaken          32  \n",
       "3          stagnation          12  \n",
       "4                 you          24  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amount_high_low_qual(df, feature, threshold):\n",
    "    high_qual = df[df[feature] > threshold]\n",
    "    low_qual = df[df[feature] < threshold]\n",
    "    return f\"high qual: {len(high_qual)}, low qual: {len(low_qual)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\urasa\\Desktop\\FYP\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_column(text):\n",
    "    return tokenizer(text, padding='max_length', truncation=True, max_length=256, return_tensors='pt')\n",
    "\n",
    "balanced_df['argument_input_ids'] = balanced_df['argument'].apply(lambda x: tokenize_column(x)['input_ids'][0])\n",
    "balanced_df['argument_attention_mask'] = balanced_df['argument'].apply(lambda x: tokenize_column(x)['attention_mask'][0])\n",
    "\n",
    "balanced_df['topic_input_ids'] = balanced_df['topic'].apply(lambda x: tokenize_column(x)['input_ids'][0])\n",
    "balanced_df['topic_attention_mask'] = balanced_df['topic'].apply(lambda x: tokenize_column(x)['attention_mask'][0])\n",
    "\n",
    "balanced_df['most_important_word_input_ids'] = balanced_df['most_important_word'].apply(lambda x: tokenize_column(x)['input_ids'][0])\n",
    "balanced_df['most_important_word_attention_mask'] = balanced_df['most_important_word'].apply(lambda x: tokenize_column(x)['attention_mask'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>argument</th>\n",
       "      <th>topic</th>\n",
       "      <th>set</th>\n",
       "      <th>WA</th>\n",
       "      <th>most_important_word</th>\n",
       "      <th>word_count</th>\n",
       "      <th>argument_input_ids</th>\n",
       "      <th>argument_attention_mask</th>\n",
       "      <th>topic_input_ids</th>\n",
       "      <th>topic_attention_mask</th>\n",
       "      <th>most_important_word_input_ids</th>\n",
       "      <th>most_important_word_attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100% agree, should they do that, it would be a...</td>\n",
       "      <td>We should abolish safe spaces</td>\n",
       "      <td>train</td>\n",
       "      <td>0.236686</td>\n",
       "      <td>number</td>\n",
       "      <td>11</td>\n",
       "      <td>[tensor(101), tensor(2531), tensor(1003), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(101), tensor(2057), tensor(2323), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(101), tensor(2193), tensor(102), tenso...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(0), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a bad score in an intelligence test is a blow ...</td>\n",
       "      <td>Intelligence tests bring more harm than good</td>\n",
       "      <td>dev</td>\n",
       "      <td>0.638716</td>\n",
       "      <td>blow</td>\n",
       "      <td>25</td>\n",
       "      <td>[tensor(101), tensor(1037), tensor(2919), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(101), tensor(4454), tensor(5852), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(101), tensor(6271), tensor(102), tenso...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(0), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A ban would be inffective, people who want a b...</td>\n",
       "      <td>Surrogacy should be banned</td>\n",
       "      <td>train</td>\n",
       "      <td>0.467092</td>\n",
       "      <td>foresaken</td>\n",
       "      <td>32</td>\n",
       "      <td>[tensor(101), tensor(1037), tensor(7221), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(101), tensor(7505), tensor(3217), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(101), tensor(18921), tensor(3736), ten...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Blockade is the perfect way to create stagna...</td>\n",
       "      <td>Blockade of the Gaza Strip should be ended</td>\n",
       "      <td>dev</td>\n",
       "      <td>0.555555</td>\n",
       "      <td>stagnation</td>\n",
       "      <td>12</td>\n",
       "      <td>[tensor(101), tensor(1037), tensor(15823), ten...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(101), tensor(15823), tensor(1997), ten...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(101), tensor(2358), tensor(8490), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A blockade is what you do when you want any ch...</td>\n",
       "      <td>Blockade of the Gaza Strip should be ended</td>\n",
       "      <td>dev</td>\n",
       "      <td>0.634427</td>\n",
       "      <td>you</td>\n",
       "      <td>24</td>\n",
       "      <td>[tensor(101), tensor(1037), tensor(15823), ten...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(101), tensor(15823), tensor(1997), ten...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(101), tensor(2017), tensor(102), tenso...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(0), t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            argument  \\\n",
       "0  100% agree, should they do that, it would be a...   \n",
       "1  a bad score in an intelligence test is a blow ...   \n",
       "2  A ban would be inffective, people who want a b...   \n",
       "3  A Blockade is the perfect way to create stagna...   \n",
       "4  A blockade is what you do when you want any ch...   \n",
       "\n",
       "                                          topic    set        WA  \\\n",
       "0                 We should abolish safe spaces  train  0.236686   \n",
       "1  Intelligence tests bring more harm than good    dev  0.638716   \n",
       "2                    Surrogacy should be banned  train  0.467092   \n",
       "3    Blockade of the Gaza Strip should be ended    dev  0.555555   \n",
       "4    Blockade of the Gaza Strip should be ended    dev  0.634427   \n",
       "\n",
       "  most_important_word  word_count  \\\n",
       "0              number          11   \n",
       "1                blow          25   \n",
       "2           foresaken          32   \n",
       "3          stagnation          12   \n",
       "4                 you          24   \n",
       "\n",
       "                                  argument_input_ids  \\\n",
       "0  [tensor(101), tensor(2531), tensor(1003), tens...   \n",
       "1  [tensor(101), tensor(1037), tensor(2919), tens...   \n",
       "2  [tensor(101), tensor(1037), tensor(7221), tens...   \n",
       "3  [tensor(101), tensor(1037), tensor(15823), ten...   \n",
       "4  [tensor(101), tensor(1037), tensor(15823), ten...   \n",
       "\n",
       "                             argument_attention_mask  \\\n",
       "0  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "1  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "2  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "3  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "4  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "\n",
       "                                     topic_input_ids  \\\n",
       "0  [tensor(101), tensor(2057), tensor(2323), tens...   \n",
       "1  [tensor(101), tensor(4454), tensor(5852), tens...   \n",
       "2  [tensor(101), tensor(7505), tensor(3217), tens...   \n",
       "3  [tensor(101), tensor(15823), tensor(1997), ten...   \n",
       "4  [tensor(101), tensor(15823), tensor(1997), ten...   \n",
       "\n",
       "                                topic_attention_mask  \\\n",
       "0  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "1  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "2  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "3  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "4  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "\n",
       "                       most_important_word_input_ids  \\\n",
       "0  [tensor(101), tensor(2193), tensor(102), tenso...   \n",
       "1  [tensor(101), tensor(6271), tensor(102), tenso...   \n",
       "2  [tensor(101), tensor(18921), tensor(3736), ten...   \n",
       "3  [tensor(101), tensor(2358), tensor(8490), tens...   \n",
       "4  [tensor(101), tensor(2017), tensor(102), tenso...   \n",
       "\n",
       "                  most_important_word_attention_mask  \n",
       "0  [tensor(1), tensor(1), tensor(1), tensor(0), t...  \n",
       "1  [tensor(1), tensor(1), tensor(1), tensor(0), t...  \n",
       "2  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "3  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "4  [tensor(1), tensor(1), tensor(1), tensor(0), t...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['argument', 'topic', 'most_important_word'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mbalanced_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43margument\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtopic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmost_important_word\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\urasa\\Desktop\\FYP\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\urasa\\Desktop\\FYP\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\urasa\\Desktop\\FYP\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\urasa\\Desktop\\FYP\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['argument', 'topic', 'most_important_word'] not found in axis\""
     ]
    }
   ],
   "source": [
    "balanced_df.drop(columns=[\"argument\", \"topic\", \"most_important_word\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  11263\n",
      "test:  3484\n",
      "val:  1735\n"
     ]
    }
   ],
   "source": [
    "train_df = balanced_df[balanced_df[\"set\"] == \"train\"]\n",
    "test_df = balanced_df[balanced_df[\"set\"] == \"test\"]\n",
    "val_df = balanced_df[balanced_df[\"set\"] == \"dev\"]\n",
    "print(\"train: \", len(train_df))\n",
    "print(\"test: \", len(test_df))\n",
    "print(\"val: \", len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'high qual: 908, low qual: 827'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amount_high_low_qual(val_df, \"labels\", 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset that inherits from the dataset class\n",
    "class ArgumentQualityDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataframe.iloc[idx]\n",
    "        return {\n",
    "            'argument_input_ids': torch.tensor(item['argument_input_ids'], dtype=torch.long),\n",
    "            'argument_attention_mask': torch.tensor(item['argument_attention_mask'], dtype=torch.long),\n",
    "            'topic_input_ids': torch.tensor(item['topic_input_ids'], dtype=torch.long),\n",
    "            'topic_attention_mask': torch.tensor(item['topic_attention_mask'], dtype=torch.long),\n",
    "            'most_important_word_input_ids': torch.tensor(item['most_important_word_input_ids'], dtype=torch.long),\n",
    "            'most_important_word_attention_mask': torch.tensor(item['most_important_word_attention_mask'], dtype=torch.long),\n",
    "            'labels': torch.tensor(item['WA'], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# custom bert model that inherits from the nn.Module class\n",
    "class CustomBertModel(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        super(CustomBertModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size * 3 + 1, 1)  \n",
    "\n",
    "    def forward(self, argument_input_ids, argument_attention_mask, topic_input_ids, topic_attention_mask, most_important_word_input_ids, most_important_word_attention_mask, word_count):\n",
    "        argument_outputs = self.bert(input_ids=argument_input_ids, attention_mask=argument_attention_mask).pooler_output\n",
    "        topic_outputs = self.bert(input_ids=topic_input_ids, attention_mask=topic_attention_mask).pooler_output\n",
    "        word_outputs = self.bert(input_ids=most_important_word_input_ids, attention_mask=most_important_word_attention_mask).pooler_output\n",
    "        \n",
    "        concatenated_outputs = torch.cat((argument_outputs, topic_outputs, word_outputs, word_count.unsqueeze(1)), dim=1)\n",
    "        logits = self.classifier(concatenated_outputs)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\urasa\\Desktop\\FYP\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = CustomBertModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ArgumentQualityDataset(train_df)\n",
    "test_dataset = ArgumentQualityDataset(test_df)\n",
    "val_dataset = ArgumentQualityDataset(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, val_dataset, test_dataset, batch_size=8, num_epochs=3, learning_rate=2e-5):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    # test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps=0, \n",
    "                                                num_training_steps=total_steps)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['argument_input_ids'].to(device)\n",
    "            attention_mask = batch['argument_attention_mask'].to(device)\n",
    "            topic_ids = batch['topic_input_ids'].to(device)\n",
    "            topic_mask = batch['topic_attention_mask'].to(device)\n",
    "            word_ids = batch['most_important_word_input_ids'].to(device)\n",
    "            word_mask = batch['most_important_word_attention_mask'].to(device)\n",
    "            word_count = batch['word_count'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(argument_input_ids=input_ids,\n",
    "                            argument_attention_mask=attention_mask,\n",
    "                            topic_input_ids=topic_ids,\n",
    "                            topic_attention_mask=topic_mask,\n",
    "                            most_important_word_input_ids=word_ids,\n",
    "                            most_important_word_attention_mask=word_mask,\n",
    "                            word_count=word_count)\n",
    "            \n",
    "            loss = loss_fn(outputs.squeeze(), labels)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_train_loss}')\n",
    "        \n",
    "        model.eval()\n",
    "        val_preds, val_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['argument_input_ids'].to(device)\n",
    "                attention_mask = batch['argument_attention_mask'].to(device)\n",
    "                topic_ids = batch['topic_input_ids'].to(device)\n",
    "                topic_mask = batch['topic_attention_mask'].to(device)\n",
    "                word_ids = batch['most_important_word_input_ids'].to(device)\n",
    "                word_mask = batch['most_important_word_attention_mask'].to(device)\n",
    "                word_count = batch['word_count'].to(device)\n",
    "                labels = batch['labels'].to(device) \n",
    "                \n",
    "                outputs = model(argument_input_ids=input_ids,\n",
    "                                argument_attention_mask=attention_mask,\n",
    "                                topic_input_ids=topic_ids,\n",
    "                                topic_attention_mask=topic_mask,\n",
    "                                most_important_word_input_ids=word_ids,\n",
    "                                most_important_word_attention_mask=word_mask,\n",
    "                                word_count=word_count)\n",
    "                \n",
    "                val_preds.extend(outputs.squeeze().cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_mse = mean_squared_error(val_labels, val_preds)\n",
    "        print(f'Validation MSE: {val_mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_dataset, batch_size=8, tolerance=0.1):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    test_preds, test_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['argument_input_ids'].to(device)\n",
    "            attention_mask = batch['argument_attention_mask'].to(device)\n",
    "            topic_ids = batch['topic_input_ids'].to(device)\n",
    "            topic_mask = batch['topic_attention_mask'].to(device)\n",
    "            word_ids = batch['most_important_word_input_ids'].to(device)\n",
    "            word_mask = batch['most_important_word_attention_mask'].to(device)\n",
    "            word_count = batch['word_count'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(argument_input_ids=input_ids,\n",
    "                            argument_attention_mask=attention_mask,\n",
    "                            topic_input_ids=topic_ids,\n",
    "                            topic_attention_mask=topic_mask,\n",
    "                            most_important_word_input_ids=word_ids,\n",
    "                            most_important_word_attention_mask=word_mask,\n",
    "                            word_count=word_count)\n",
    "            \n",
    "            test_preds.extend(outputs.squeeze().cpu().numpy())\n",
    "            test_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "    test_preds = np.array(test_preds)\n",
    "    test_labels = np.array(test_labels)\n",
    "    \n",
    "    acc_within_tolerance = np.abs(test_preds - test_labels) <= tolerance\n",
    "    acc_within_tolerance = np.mean(acc_within_tolerance)\n",
    "    \n",
    "    test_mse = mean_squared_error(test_labels, test_preds)\n",
    "    test_mae = mean_absolute_error(test_labels, test_preds)\n",
    "    test_r2 = r2_score(test_labels, test_preds)\n",
    "    \n",
    "    print(f'Test MSE: {test_mse}')\n",
    "    print(f'Test MAE: {test_mae}')\n",
    "    print(f'Test R²: {test_r2}')\n",
    "    print(f'Accuracy within {tolerance} tolerance: {acc_within_tolerance}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
